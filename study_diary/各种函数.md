# 各种函数

## 1.基本概念

* 损失函数：计算的是一个样本的误差
* 代价函数：是整个训练集上所有样本误差的平均
* 目标函数：代价函数 + 正则化项

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta^*&space;=&space;\arg&space;\min_\theta&space;\frac{1}{N}{}\sum_{i=1}^{N}&space;L(y_i,&space;f(x_i;&space;\theta))&space;&plus;&space;\lambda\&space;\Phi(\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta^*&space;=&space;\arg&space;\min_\theta&space;\frac{1}{N}{}\sum_{i=1}^{N}&space;L(y_i,&space;f(x_i;&space;\theta))&space;&plus;&space;\lambda\&space;\Phi(\theta)" title="\theta^* = \arg \min_\theta \frac{1}{N}{}\sum_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\ \Phi(\theta)" /></a>

损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分。模型的风险结构包括了风险项和正则项；

前面的均值函数表示的是经验风险函数，LL代表的是损失函数，后面的 ΦΦ 是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是**找到使目标函数最小时的θ值**

## 2. 常用损失函数

1. hinge loss: 主要用于支持向量机（SVM） 中；

   Hinge loss 的叫法来源于其损失函数的图形，为一个折线，通用的函数表达式为：

   表示如果被正确分类，损失是0，否则损失就是

   <a href="https://www.codecogs.com/eqnedit.php?latex=L(m_i)&space;=&space;max(0,1-m_i(w))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(m_i)&space;=&space;max(0,1-m_i(w))" title="L(m_i) = max(0,1-m_i(w))" /></a>

2. softmax Loss: 

   在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即maxF(y,f(x))→min−F(y,f(x)))maxF(y,f(x))→min−F(y,f(x)))。从损失函数的视角来看，它就成了Softmax 损失函数了。

3. squared loss

   最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（中心极限定理），最后通过**极大似然估计**（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：**最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小**。我们的目标就是最小化这个目标函数值，即**最小化残差的平方和**。

4. exponentially loss

   损失函数的标准形式是： 
   L(Y,f(X))=exp[−Yf(X)]
   L(Y,f(X))=exp⁡[−Yf(X)]

   exp-loss，主要应用于 Boosting 算法中，在Adaboost 算法中，经过 mm 次迭代后，可以得到 fm(x)fm(x) 

5. 其他损失

   0-1损失

   绝对值损失